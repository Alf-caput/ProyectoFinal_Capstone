{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2955216,"sourceType":"datasetVersion","datasetId":1811884}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-19T16:15:59.583125Z","iopub.execute_input":"2024-05-19T16:15:59.583869Z","iopub.status.idle":"2024-05-19T16:16:00.859143Z","shell.execute_reply.started":"2024-05-19T16:15:59.583834Z","shell.execute_reply":"2024-05-19T16:16:00.858018Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/stanford-earthquake-dataset-stead/merge.hdf5\n/kaggle/input/stanford-earthquake-dataset-stead/merge.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Para trabajar con el dataset, utilizamos los servicios de kaggle que nos permiten guardar el input de manera persistente, lo cual es conveniente al ser un dataset de ~90GB.","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\n\ninput_dir = '/kaggle/input/stanford-earthquake-dataset-stead/'\nfilename = 'merge.csv'\ndf_original = pd.read_csv(os.path.join(input_dir, filename), low_memory=False)\ndf_original.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T16:16:00.862058Z","iopub.execute_input":"2024-05-19T16:16:00.862928Z","iopub.status.idle":"2024-05-19T16:16:32.230947Z","shell.execute_reply.started":"2024-05-19T16:16:00.862887Z","shell.execute_reply":"2024-05-19T16:16:32.229670Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"  network_code receiver_code receiver_type  receiver_latitude  \\\n0           TA          109C            HH            32.8889   \n1           TA          109C            HH            32.8889   \n2           TA          109C            HH            32.8889   \n3           TA          109C            HH            32.8889   \n4           TA          109C            HH            32.8889   \n\n   receiver_longitude  receiver_elevation_m  p_arrival_sample p_status  \\\n0           -117.1051                 150.0               NaN      NaN   \n1           -117.1051                 150.0               NaN      NaN   \n2           -117.1051                 150.0               NaN      NaN   \n3           -117.1051                 150.0               NaN      NaN   \n4           -117.1051                 150.0               NaN      NaN   \n\n   p_weight  p_travel_sec  ...  source_magnitude_author  \\\n0       NaN           NaN  ...                      NaN   \n1       NaN           NaN  ...                      NaN   \n2       NaN           NaN  ...                      NaN   \n3       NaN           NaN  ...                      NaN   \n4       NaN           NaN  ...                      NaN   \n\n  source_mechanism_strike_dip_rake  source_distance_deg source_distance_km  \\\n0                              NaN                  NaN                NaN   \n1                              NaN                  NaN                NaN   \n2                              NaN                  NaN                NaN   \n3                              NaN                  NaN                NaN   \n4                              NaN                  NaN                NaN   \n\n  back_azimuth_deg  snr_db  coda_end_sample     trace_start_time  \\\n0              NaN     NaN              NaN  2015-10-21 05:55:00   \n1              NaN     NaN              NaN  2015-11-06 14:50:00   \n2              NaN     NaN              NaN  2015-11-07 02:20:00   \n3              NaN     NaN              NaN  2015-11-14 05:15:00   \n4              NaN     NaN              NaN  2015-12-25 18:50:00   \n\n   trace_category               trace_name  \n0           noise  109C.TA_201510210555_NO  \n1           noise  109C.TA_201511061450_NO  \n2           noise  109C.TA_201511070220_NO  \n3           noise  109C.TA_201511140515_NO  \n4           noise  109C.TA_201512251850_NO  \n\n[5 rows x 35 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>network_code</th>\n      <th>receiver_code</th>\n      <th>receiver_type</th>\n      <th>receiver_latitude</th>\n      <th>receiver_longitude</th>\n      <th>receiver_elevation_m</th>\n      <th>p_arrival_sample</th>\n      <th>p_status</th>\n      <th>p_weight</th>\n      <th>p_travel_sec</th>\n      <th>...</th>\n      <th>source_magnitude_author</th>\n      <th>source_mechanism_strike_dip_rake</th>\n      <th>source_distance_deg</th>\n      <th>source_distance_km</th>\n      <th>back_azimuth_deg</th>\n      <th>snr_db</th>\n      <th>coda_end_sample</th>\n      <th>trace_start_time</th>\n      <th>trace_category</th>\n      <th>trace_name</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>TA</td>\n      <td>109C</td>\n      <td>HH</td>\n      <td>32.8889</td>\n      <td>-117.1051</td>\n      <td>150.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-10-21 05:55:00</td>\n      <td>noise</td>\n      <td>109C.TA_201510210555_NO</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>TA</td>\n      <td>109C</td>\n      <td>HH</td>\n      <td>32.8889</td>\n      <td>-117.1051</td>\n      <td>150.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-11-06 14:50:00</td>\n      <td>noise</td>\n      <td>109C.TA_201511061450_NO</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>TA</td>\n      <td>109C</td>\n      <td>HH</td>\n      <td>32.8889</td>\n      <td>-117.1051</td>\n      <td>150.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-11-07 02:20:00</td>\n      <td>noise</td>\n      <td>109C.TA_201511070220_NO</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>TA</td>\n      <td>109C</td>\n      <td>HH</td>\n      <td>32.8889</td>\n      <td>-117.1051</td>\n      <td>150.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-11-14 05:15:00</td>\n      <td>noise</td>\n      <td>109C.TA_201511140515_NO</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>TA</td>\n      <td>109C</td>\n      <td>HH</td>\n      <td>32.8889</td>\n      <td>-117.1051</td>\n      <td>150.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2015-12-25 18:50:00</td>\n      <td>noise</td>\n      <td>109C.TA_201512251850_NO</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 35 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Realizamos una copia del dataset, para ejecutar desde este punto las celdas, y no tener que volver a leer el dataset a pandas","metadata":{}},{"cell_type":"code","source":"df = df_original.copy()\ndf.info()","metadata":{"execution":{"iopub.status.busy":"2024-05-19T18:15:12.045574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se trata de un dataset con 1265657 filas y 35 columnas.","metadata":{}},{"cell_type":"markdown","source":"La variable trace_name es un identificador único para cada observación:","metadata":{}},{"cell_type":"code","source":"len(df['trace_name'].unique()) == len(df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Por tanto, lo podemos utilizar como índice de fila.","metadata":{}},{"cell_type":"code","source":"df = df.set_index('trace_name')\ndf.head()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"network_code, receiver_code, trace_start_time y trace_category se especifican en el trace_name  \n\ntrace_name = [receiver_code].[network_code]\\_[trace_start_time]\\_[trace_category]\n\nObservaciones:\n- trace_start_time se expresa como unión sin espacios de las unidades de tiempo\n- trace_category toma valores NO y EV para categorías noise y earthquake_local respectivamente","metadata":{}},{"cell_type":"markdown","source":"Se convierten a formato fecha las variables source_origin_time y trace_start_time","metadata":{}},{"cell_type":"code","source":"df['trace_start_time'] = pd.to_datetime(df['trace_start_time'], format='ISO8601')# format=\"%Y-%m-%d %H:%M:%S\")\ndf['source_origin_time'] = pd.to_datetime(df['source_origin_time'], format='ISO8601')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[0]['trace_start_time']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Se han recogido dos tipos de señales:  \n- noise: sonidos no debidos a terremotos\n- earthquake_local: terremotos que se encuentran en un radio menor a 350km","metadata":{}},{"cell_type":"code","source":"df.loc[:, 'trace_category'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Para estudios que vamos a realizar, vamos dividir en dataframes uno por cada tipo de señal:","metadata":{}},{"cell_type":"markdown","source":"### Señales noise","metadata":{}},{"cell_type":"markdown","source":"Obtenemos un subdataset cuyas observaciones son señales noise, y podemos ver el ratio y el tamaño del subconjunto","metadata":{}},{"cell_type":"code","source":"df_noise = df.loc[df.loc[:, 'trace_category'] == 'noise']\nsize = len(df_noise)\nprint(f'len(df_noise)={size}, ratio={size/len(df)*100:.2f}%')\ndf_noise.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"En la documentación del dataset se muestra que las señales categorizadas como noise, solo cuentan con 8 variables y el resto toman valores NA, eliminamos las variables que tienen todos sus valores NA:","metadata":{}},{"cell_type":"code","source":"df_noise = df_noise.dropna(axis=1, how='all')\nprint(df_noise.shape)\ndf_noise.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Una vez hecho esto, podemos comprobar si existen más valores NA, debidos a incosistencias.","metadata":{}},{"cell_type":"code","source":"print(f'Número total de NA en el dataframe: {(col_na:=df_noise.isna().sum()).sum()}')\ndf_noise_na = pd.DataFrame({'Valores NA': col_na})\ndf_noise_na","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Realizamos comprobaciones similares para los datos de terremotos.","metadata":{}},{"cell_type":"code","source":"df_earthquakes = df.loc[df.loc[:, 'trace_category'] == 'earthquake_local']\nsize = len(df_earthquakes)\nprint(f'len(df_earthquakes)={size}, ratio={size/len(df)*100:.2f}%')\ndf_earthquakes.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f'Número total de NA en el dataframe: {(col_na:=df_earthquakes.isna().sum()).sum()}')\ndf_earthquakes_na = pd.DataFrame({'Valores NA': col_na})\ndf_earthquakes_na.loc[df_earthquakes_na.loc[:, 'Valores NA'] != 0, :] # Mostramos solo los que no tienen 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_earthquakes = df_earthquakes.dropna(axis=1, subset=['network_code', 'p_weight', 's_weight', 'source_depth_km'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nna_data = df_earthquakes_na.loc[df_earthquakes_na.loc[:, 'Valores NA'] != 0, :]\nplt.figure(figsize=(10, 6))\nplt.hist(na_data, bins=df_earthquakes_na.columns, alpha=0.7, rwidth=0.85)\nplt.xlabel('Valor')\nplt.ylabel('Frecuencia')\nplt.title('Histograma de los valores de una única columna')\nplt.grid(axis='y', alpha=0.75)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# reemplazar NaN con la media de la columna correspondiente\n#imputer = SimpleImputer(strategy='mean')\n\n# Preparación de los datos\nX = df_earthquakes[['source_latitude', 'source_longitude', 'source_depth_km']]  # predictores\ny = df_earthquakes['source_magnitude']  # var objetivo\n\nX.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# reemplazar NaN con la media de la columna correspondiente\n#imputer = SimpleImputer(strategy='mean')\n\n# Preparación de los datos\nX = df_earthquakes[['source_latitude', 'source_longitude', 'source_depth_km']]  # predictores\ny = df_earthquakes['source_magnitude']  # var objetivo\n\n# Imputar valores NaN\n# X = imputer.fit_transform(X)\n\n#datos en entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Crear y entrenar el modelo de regresión lineal\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Predecir el conjunto de prueba\ny_pred = model.predict(X_test)\n\n# las métricas de rendimiento\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f'MSE: {mse}')\nprint(f'R^2: {r2}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Podemos comprobar mediante source_id como se distribuyen el número de muestras que hacen referencia al mismo terremoto","metadata":{}},{"cell_type":"code","source":"earthquake_samples = df_earthquakes.groupby('source_id')\nsizes = earthquake_samples.size()\nsizes.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"df_earthquakes.groupby('source_magnitude_type').size().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_earthquakes.groupby('source_magnitude').size().plot(kind='bar')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}